{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74a1ac06",
   "metadata": {},
   "source": [
    "# The Everything Bagel GNN\n",
    "Welcome to the comprehensive tutorial on \"The Everything Bagel GNN,\" a cutting-edge approach to multimodal graph neural networks (GNNs) empowered by the [GRAPE library](https://github.com/AnacletoLAB/grape). In this tutorial, we will embark on a fascinating journey, exploring the vast capabilities of this state-of-the-art GNN architecture implemented in Rust and TensorFlow, with Python bindings.\n",
    "\n",
    "Graphs are pervasive in various domains, representing intricate relationships and connections between entities. However, traditional GNNs often struggle to effectively capture the richness and complexity of multimodal graphs that incorporate diverse node types, edge types, node embeddings, and node features.\n",
    "\n",
    "The Everything Bagel GNN offers a comprehensive solution to leverage all available modalities. With this powerful general GNN architecture, you can seamlessly integrate different convolutional kernels, including right Laplacian, left Laplacian, and transposed Laplacian among others, enabling customized graph convolutions tailored to your specific analysis needs. [Learn more about using multiple kernels in GNNs in this paper](https://arxiv.org/pdf/2305.10498.pdf).\n",
    "\n",
    "But that's just the beginning! We go beyond convolutional kernels and dive into the realm of multimodality. The Everything Bagel GNN enables the fusion of node type, edge type, and node embedding information, or node type, edge type, and node features, providing a holistic view of the graph's intricate relationships and properties. This comprehensive approach empowers you to extract deeper insights and uncover hidden patterns within your graph data.\n",
    "\n",
    "Another thing that sets the Everything Bagel GNN apart is its unique integration of subgraph sketching-based edge features. Leveraging the advanced capabilities of GRAPE, you can now efficiently incorporate subgraph sketching techniques to capture rich structural information in the form of edge features. By doing so, you enhance the representation power of your GNN, enabling more accurate predictions, improved link prediction, and enhanced graph analysis capabilities. [Learn more about graph sketching in the original paper](https://openreview.net/pdf?id=m1oqEOAozQU) and [find the relative tutorial here](https://github.com/AnacletoLAB/grape/blob/main/tutorials/Link%20prediction%20models%20using%20subgraph%20sketching%20using%20GRAPE.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7816061",
   "metadata": {},
   "source": [
    "## Installing all requirements\n",
    "To get started with the tutorial and use GRAPE, we need to install the necessary requirements. Please run the following command in your terminal or notebook cell to install the required packages:\n",
    "\n",
    "```bash\n",
    "pip install grape torch transformers tensorflow silence_tensorflow -qU\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2534ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install grape torch transformers tensorflow silence_tensorflow -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e790b",
   "metadata": {},
   "source": [
    "We use [Silence tensorflow](https://github.com/LucaCappelletti94/silence_tensorflow) to shut up the extra verbose and rather useless warnings from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import silence_tensorflow.auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b0ff6",
   "metadata": {},
   "source": [
    "I want to only use the CPU for this tutorial, as I do not have CUDA properly configured on this machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0e77bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75daaa8",
   "metadata": {},
   "source": [
    "Make sure that the version of GRAPE and TensorFlow you have installed is compatible with the ones below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce31da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grape import print_version\n",
    "print_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7088088c",
   "metadata": {},
   "source": [
    "## Retrieving the data\n",
    "Throughout this tutorial, we will be working with two knowledge graphs available from KGOBO and KGHub. For both graph, we are going to use the associated [BioLink](https://biolink.github.io/biolink-model/) metadata as features for the node types and edge types - specifically, we are going to use the pre-computed BERT and DeepWalk embedding associated to the BioLink descriptions and topology, respectively. Subsequently. we are going to compute the Okapi TFIDF SciBERT embeddings of the node descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31d813",
   "metadata": {},
   "source": [
    "### Human Phenotype Ontology\n",
    "The Human Phenotype Ontology (HPO) is a standardized vocabulary of phenotypic abnormalities and their related annotations. It provides a structured and comprehensive representation of human phenotypes, facilitating the analysis and interpretation of genetic and genomic data. [Learn more about this data here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2668030/).\n",
    "\n",
    "HPO is by far the smaller of the two graphs, and we wil use it as example for the most complex version of the Everything Bagel. Specifically, we are going to consider its directionality, and use several graph convolution kernels to properly model it in the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db378ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grape.datasets.kgobo import HP\n",
    "hpo = HP(version='2023-01-27', directed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fbf914",
   "metadata": {},
   "source": [
    "We compute the graph report - please note that directed graph have a shorter report, and when converted to an undirected graph you can get the more extensive one. This happens because several algorithms become much less efficient when working on directed graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee1c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac9e85",
   "metadata": {},
   "source": [
    "Since there are singletons in the graph, we drop them. These nodes are most likely deprecated entities from previous versions. Also, since TensorFlow sparse vector does not support multigraphs, we need to drop the multigraph edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_hpo = hpo.remove_disconnected_nodes()\\\n",
    "    .remove_parallel_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52f020f",
   "metadata": {},
   "source": [
    "We re-run the report and observe whether everything looks nominal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaded25",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_hpo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c7f19",
   "metadata": {},
   "source": [
    "### KGCOVID19\n",
    "KGCOVID19 is a knowledge graph that aggregates and integrates various data sources related to the COVID-19 pandemic. [Learn more about this data here](https://www.cell.com/patterns/fulltext/S2666-3899(20)30203-8?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS2666389920302038%3Fshowall%3Dtrue). KGCOVID is much larger than HPO, and requires more attention to scalability. To make sure we can work on this graph, we will avoid using trainable graph convolutions but, [as done in this paper](https://openreview.net/pdf?id=m1oqEOAozQU), we pre-compute the convolutions using the left laplacian. Furthermore, to reduce the dimensionality of these features, we use a [PCA Decomposition](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b4539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from grape.datasets.kghub import KGCOVID19\n",
    "kgcovid19 = KGCOVID19(version=\"20230102\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db43347",
   "metadata": {},
   "source": [
    "We compute the report of the KGCOVID19 graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea01b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kgcovid19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6840b86c",
   "metadata": {},
   "source": [
    "The graph comes with several topological oddities which we need to clean up before running some graph ML on it. For starters, we remove all singletons and smaller components, keeping only the larger one. Next, we remove parallel edges, as TensorFlow's sparse tensor representation does not support multigraphs. Finally, we remove the dendritic trees, i.e. all portions of the graph with sparsity equal to a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filtered_kgcovid19 = kgcovid19.remove_components(top_k_components=1)\\\n",
    "    .remove_parallel_edges()\\\n",
    "    .remove_dendritic_trees()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083a4522",
   "metadata": {},
   "source": [
    "We display the report associated to the filtered version of KGCOVID19:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5573f1e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filtered_kgcovid19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73bab53",
   "metadata": {},
   "source": [
    "## Some helper functions\n",
    "We are going to use the pre-computed BioLink [topological](https://github.com/LucaCappelletti94/kg-biolink) and [SciBERT](https://github.com/LucaCappelletti94/biolink_embedding) embeddings. As often happens with standards, there is some dis-alignment between the version used in KGCOVID19 and the one currently available from the biolink reference, so we need to normalize the two, hence the need of the two following helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ff7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "from grape import Graph\n",
    "\n",
    "def get_node_type_features(graph: Graph, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return the node type features associated to the provided graph from the provided dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ---------------\n",
    "    graph: Graph\n",
    "        The graph whose node types are to be normalized and queries\n",
    "    df: pd.DataFrame\n",
    "        The dataframe from which to get the node type features.\n",
    "    \"\"\"\n",
    "    remap = {\n",
    "        \"rna\": \"RNAProduct\",\n",
    "        \"assay\": \"procedure\"\n",
    "    }\n",
    "    df = df.copy()\n",
    "    df.index = [\n",
    "        \"\".join([\n",
    "            term.lower()\n",
    "            for term in term.split(\" \")\n",
    "        ])\n",
    "        for term in df.index\n",
    "    ]\n",
    "    df = df.loc[[\n",
    "        remap.get(\n",
    "            node_type_name.split(\":\")[1].lower(),\n",
    "            node_type_name.split(\":\")[1].lower()\n",
    "        ).lower()\n",
    "        for node_type_name in graph.get_unique_node_type_names()\n",
    "    ]]\n",
    "    df.index = graph.get_unique_node_type_names()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a505a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "from grape import Graph\n",
    "\n",
    "def get_edge_type_features(graph: Graph, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return the edge type features associated to the provided graph from the provided dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ---------------\n",
    "    graph: Graph\n",
    "        The graph whose edge types are to be normalized and queries\n",
    "    df: pd.DataFrame\n",
    "        The dataframe from which to get the edge type features.\n",
    "    \"\"\"\n",
    "    remap = {\n",
    "        \"positively_regulates\": \"increases_amount_or_activity_of\",\n",
    "        \"negatively_regulates\": \"decreases_amount_or_activity_of\",\n",
    "        \"positivelyregulates\": \"increases_amount_or_activity_of\",\n",
    "        \"negativelyregulates\": \"decreases_amount_or_activity_of\",\n",
    "        \"inverseof\": \"opposite_of\",\n",
    "        \"subpropertyof\": \"member_of\",\n",
    "        \"affectstransportof\": \"affects\",\n",
    "        \"affects_transport_of\": \"affects\",\n",
    "        \"increases_degradation_of\": \"decreased_amount_in\",\n",
    "        \"affects_localization_of\": \"affects\",\n",
    "        \"negativelyregulateprocesstoprocess\": \"decreases_amount_or_activity_of\",\n",
    "        \"molecularly_interacts_with\": \"physically_interacts_with\",\n",
    "        \"regulateprocesstoprocess\": \"affects\",\n",
    "    }\n",
    "    df = df.copy()\n",
    "    df.index = [\n",
    "        \"_\".join([\n",
    "            term.lower()\n",
    "            for term in term.split(\" \")\n",
    "        ])\n",
    "        for term in df.index\n",
    "    ]\n",
    "    df = df.loc[[\n",
    "        remap.get(edge_type_name.split(\":\")[1].lower(), edge_type_name.split(\":\")[1].lower()).lower()\n",
    "        for edge_type_name in graph.get_unique_edge_type_names()\n",
    "    ]]\n",
    "    df.index = graph.get_unique_edge_type_names()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe029b48",
   "metadata": {},
   "source": [
    "We retrieve the precomputed BioLink [SciBERT](https://arxiv.org/pdf/1903.10676.pdf) embeding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f4fc05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "biolink_bert = pd.read_csv(\n",
    "    \"https://github.com/LucaCappelletti94/biolink_embedding/raw/main/\"\n",
    "    \"biolink_3.4.3_allenai_scibert_scivocab_uncased.csv.gz\",\n",
    "    compression='gzip',\n",
    "    index_col=[0]\n",
    ")\n",
    "\n",
    "biolink_bert.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e593137d",
   "metadata": {},
   "source": [
    "We query the SciBERT node type features of HPO and KGCOVID19 - we can do this because the two graphs have node type and edge types that adhere to the BioLink standard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c9e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "node_type_bert_embedding_hpo = get_node_type_features(filtered_hpo, biolink_bert)\n",
    "node_type_bert_embedding_hpo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fd2271",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "node_type_bert_embedding_kgcovid19 = get_node_type_features(filtered_kgcovid19, biolink_bert)\n",
    "node_type_bert_embedding_kgcovid19.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "edge_type_bert_embedding_hpo = get_edge_type_features(filtered_hpo, biolink_bert)\n",
    "edge_type_bert_embedding_hpo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e5e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "edge_type_bert_embedding_kgcovid19 = get_edge_type_features(filtered_kgcovid19, biolink_bert)\n",
    "edge_type_bert_embedding_kgcovid19.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad790196",
   "metadata": {},
   "source": [
    "Since BioLink describes the topological relationship between the various BioLink classes, we can [build a graph out of it](https://github.com/LucaCappelletti94/kg-biolink). Since we can build a graph, we can embed it. Since we can embed it, we have in addition to the textual features also topological features for all node types and edge types - Specifically, we employed [DeepWalk](https://arxiv.org/pdf/1403.6652.pdf) to embed the graph.\n",
    "\n",
    "As done earlier, we will query the dataframe to get the precomputed DeepWalk topological features for the node types and edge types of the two graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "biolink_deepwalk = pd.read_csv(\n",
    "    \"https://github.com/LucaCappelletti94/kg-biolink/raw/main/\"\n",
    "    \"kg_biolink_deepwalk_center.csv.gz\",\n",
    "    compression='gzip',\n",
    "    index_col=[0]\n",
    ")\n",
    "\n",
    "biolink_deepwalk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4993e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "node_type_deepwalk_embedding_hpo = get_node_type_features(filtered_hpo, biolink_deepwalk)\n",
    "node_type_deepwalk_embedding_hpo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6629e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "node_type_deepwalk_embedding_kgcovid19 = get_node_type_features(filtered_kgcovid19, biolink_deepwalk)\n",
    "node_type_deepwalk_embedding_kgcovid19.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdb18d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "edge_type_deepwalk_embedding_hpo = get_edge_type_features(filtered_hpo, biolink_deepwalk)\n",
    "edge_type_deepwalk_embedding_hpo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "edge_type_deepwalk_embedding_kgcovid19 = get_edge_type_features(filtered_kgcovid19, biolink_deepwalk)\n",
    "edge_type_deepwalk_embedding_kgcovid19.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f37e968",
   "metadata": {},
   "source": [
    "## Okapi BM25 SciBERT node features\n",
    "We now proceed to compute the [Okapi BM25 SciBERT node features](https://github.com/LucaCappelletti94/pubmed_embedding/blob/main/BM25_weighted_BERT_based_embedding_of_PubMed.pdf), which are pretty much analogous to what was done for the precomputed BioLink features - Here I will show you step by step how to do it, and as you will see it is quite easy to do using the pipeline I have prepared for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5800831c",
   "metadata": {},
   "source": [
    "First, we make sure that there is indeed textual data in the file associated to the nodes. In the case of both KGCOVID19 and HPO, we have decent descriptions of what the nodes represent - the more extensive, the better. We proceed to run an Okabi BM25 TFID ranking function to weight the single tokens in each of the rows, and we use the weights to compute a weighted average of the pretrained SciBERT embedding associated to each of the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccaa81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "kgcovid19_node_path = \"/bfd/graphs/kghub/KGCOVID19/20230102/kg-covid-19/merged-kg_nodes.tsv\"\n",
    "pd.read_csv(kgcovid19_node_path, nrows=20, sep=\"\\t\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d941cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "hpo_node_path = \"/bfd/graphs/kgobo/HP/2023-01-27/hp_kgx_tsv/hp_kgx_tsv_nodes.tsv\"\n",
    "pd.read_csv(hpo_node_path, nrows=20, sep=\"\\t\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4156bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from grape.datasets import get_okapi_tfidf_weighted_textual_embedding\n",
    "\n",
    "node_bert_embedding_kgcovid19 = pd.DataFrame(\n",
    "    get_okapi_tfidf_weighted_textual_embedding(\n",
    "        kgcovid19_node_path,\n",
    "        pretrained_model_name_or_path=\"allenai/scibert_scivocab_uncased\"\n",
    "    ),\n",
    "    # Since there is a row for each of the graph nodes, we need to use the complete\n",
    "    # graph and not the filtered version of the graph. Afterwards, we query for the\n",
    "    # subset of nodes which we actually care about.\n",
    "    index=kgcovid19.get_node_names()\n",
    ").loc[filtered_kgcovid19.get_node_names()]\n",
    "node_bert_embedding_kgcovid19.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f84936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from grape.datasets import get_okapi_tfidf_weighted_textual_embedding\n",
    "\n",
    "node_bert_embedding_hpo = pd.DataFrame(\n",
    "    get_okapi_tfidf_weighted_textual_embedding(\n",
    "        hpo_node_path,\n",
    "        pretrained_model_name_or_path=\"allenai/scibert_scivocab_uncased\"\n",
    "    ),\n",
    "    # Since there is a row for each of the graph nodes, we need to use the complete\n",
    "    # graph and not the filtered version of the graph. Afterwards, we query for the\n",
    "    # subset of nodes which we actually care about.\n",
    "    index=hpo.get_node_names()\n",
    ").loc[filtered_hpo.get_node_names()]\n",
    "node_bert_embedding_hpo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf960b5",
   "metadata": {},
   "source": [
    "### Graph visualization\n",
    "We use the graph visualization toolkit available from GRAPE to see whether the computed node features are meaningful for the current graph topologies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd05d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grape import GraphVisualizer\n",
    "\n",
    "GraphVisualizer(filtered_hpo).fit_and_plot_all(node_bert_embedding_hpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594605ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grape import GraphVisualizer\n",
    "\n",
    "GraphVisualizer(filtered_kgcovid19).fit_and_plot_all(node_bert_embedding_kgcovid19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2c4c38",
   "metadata": {},
   "source": [
    "### PCA dimensionality reduction\n",
    "Since the BERT features have a rather large dimension (768) and the KGCOVID19 graph is not a small graph, we reduce the dimensionality of the features using [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) so to make it feaseable to use these features even on more modest hardware. We will reduce the dimensionality of the features down to `50`. This step is not necessary if your hardware allows to use larger features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7ab104",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "node_bert_embedding_kgcovid19_pca = pd.DataFrame(\n",
    "    pca.fit_transform(node_bert_embedding_kgcovid19),\n",
    "    index=node_bert_embedding_kgcovid19.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ef708",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_bert_embedding_kgcovid19_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b2fa70",
   "metadata": {},
   "source": [
    "We visualize again the node features after the PCA procedure, so to make sure we did not destroy to much information with the dimensionality reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dab6932",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from grape import GraphVisualizer\n",
    "\n",
    "GraphVisualizer(filtered_kgcovid19).fit_and_plot_all(node_bert_embedding_kgcovid19_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b93d82",
   "metadata": {},
   "source": [
    "Since the number of node types and edge types is actually less than the number of target components, we are going to run the PCA on the full set of node type and edge type features, and query them afterwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b689f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "\n",
    "biolink_bert_pca = pd.DataFrame(\n",
    "    pca.fit_transform(biolink_bert),\n",
    "    index=biolink_bert.index\n",
    ")\n",
    "\n",
    "biolink_deepwalk_pca = pd.DataFrame(\n",
    "    pca.fit_transform(biolink_deepwalk),\n",
    "    index=biolink_deepwalk.index\n",
    ")\n",
    "\n",
    "node_type_bert_embedding_kgcovid19_pca = get_node_type_features(filtered_kgcovid19, biolink_bert_pca)\n",
    "node_type_deepwalk_embedding_kgcovid19_pca = get_node_type_features(filtered_kgcovid19, biolink_deepwalk_pca)\n",
    "edge_type_bert_embedding_kgcovid19_pca = get_edge_type_features(filtered_kgcovid19, biolink_bert_pca)\n",
    "edge_type_deepwalk_embedding_kgcovid19_pca = get_edge_type_features(filtered_kgcovid19, biolink_deepwalk_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f629dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type_bert_embedding_kgcovid19_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae91c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type_deepwalk_embedding_kgcovid19_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_type_bert_embedding_kgcovid19_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_type_deepwalk_embedding_kgcovid19_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0441f4",
   "metadata": {},
   "source": [
    "### HERE WE START TO USE THE GRAPHS TOPOLOGY!\n",
    "Please pay attention that here we are about to start use the graph topology. For this reason, at this point, we execute the graph holdouts that split the training and test edges. Until now, we did not use the graph topology at any point, so we have not introduced any bias while during the previous operations. [We are going to create connected holdouts - you can learn more about graph holdouts in this previous tutorial](https://github.com/AnacletoLAB/grape/blob/main/tutorials/Graph_holdouts_using_GRAPE.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_kgcovid19, test_kgcovid19 = filtered_kgcovid19.connected_holdout(\n",
    "    train_size=0.8,\n",
    "    random_state=4435,\n",
    ")\n",
    "train_kgcovid19 = train_kgcovid19.add_selfloops(edge_type_name=\"biolink:related_to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efc6d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_hpo, test_hpo = filtered_hpo.connected_holdout(\n",
    "    train_size=0.8,\n",
    "    random_state=4435,\n",
    ")\n",
    "train_hpo = train_hpo.add_selfloops(edge_type_name=\"biolink:related_to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bacd38",
   "metadata": {},
   "source": [
    "### Precomputing KGCOVID19 graph convolution\n",
    "As aforementioned, for the KGCOVID19 graph instead than using trainable graph convolution layers, we are going to pre-compute `3` convolution steps, [as described in this paper](https://openreview.net/pdf?id=m1oqEOAozQU). We will emulate residual layers, by concatenating the result of each convolution and obtaining therefore a `50*(3+1) = 200` dimensional set of node features. Note that **we are using the training graph only, and not the test graph**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb50fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from grape.feature_preprocessors import GraphConvolution\n",
    "\n",
    "conv = GraphConvolution(\n",
    "    number_of_convolutions=3,\n",
    "    concatenate_features=True\n",
    ")\n",
    "\n",
    "conv_node_bert_embedding_kgcovid19_pca = pd.DataFrame(\n",
    "    conv.transform(\n",
    "        support=train_kgcovid19,\n",
    "        node_features=node_bert_embedding_kgcovid19_pca.values\n",
    "    )[0],\n",
    "    index=node_bert_embedding_kgcovid19_pca.index\n",
    ")\n",
    "\n",
    "conv_node_bert_embedding_kgcovid19_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9537f",
   "metadata": {},
   "source": [
    "Again, we visualize the node features after the procedure, to see if it has changed them and how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf608c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from grape import GraphVisualizer\n",
    "\n",
    "GraphVisualizer(train_kgcovid19,).fit_and_plot_all(conv_node_bert_embedding_kgcovid19_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941a0a2",
   "metadata": {},
   "source": [
    "## Subgraph sketching\n",
    "Subgraph sketching is a technique in graph analysis that captures the local connectivity patterns of subgraphs within a larger graph. By leveraging HyperLogLog counters, subgraph sketching allows for efficient estimation of neighbours intersections at different distances. We use it to compute edge embeddings.\n",
    "\n",
    "[Learn more in its dedicated tutorial](https://github.com/AnacletoLAB/grape/blob/main/tutorials/Link%20prediction%20models%20using%20subgraph%20sketching%20using%20GRAPE.ipynb) and [the original paper presenting it](https://openreview.net/pdf?id=m1oqEOAozQU).\n",
    "\n",
    "\n",
    "The gist of it, is that we will compute the cardinality of the sets highlighted in the following pictures using [efficient HyperLogLog counters](https://github.com/LucaCappelletti94/hyperloglog-rs). The vector of cardinalities will be the sketch representing a given tuple of nodes.\n",
    "\n",
    "![triple overlap](https://github.com/LucaCappelletti94/hyperloglog-rs/blob/main/triple_overlap.png?raw=true)\n",
    "\n",
    "Do note that we do not precompute the sketching features for all possible edges, as that would never fit in memory, but we compute them on stream as the model is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565417d",
   "metadata": {},
   "source": [
    "As done earlier, we start by displaying these features. Do note that since these are ONLY EDGE FEATURES the visualization will not include any plot that involves the nodes, but only the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443bc6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grape.embedders import HyperSketching\n",
    "\n",
    "GraphVisualizer(train_hpo).fit_and_plot_all(HyperSketching(\n",
    "    # We execute six hops of intersections.\n",
    "    number_of_hops=6\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grape.embedders import HyperSketching\n",
    "\n",
    "GraphVisualizer(test_hpo, support=train_hpo).fit_and_plot_all(HyperSketching(\n",
    "    # We execute six hops of intersections.\n",
    "    number_of_hops=6\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0abc789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grape.embedders import HyperSketching\n",
    "\n",
    "GraphVisualizer(train_kgcovid19).fit_and_plot_all(HyperSketching(\n",
    "    # We execute six hops of intersections.\n",
    "    number_of_hops=6\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6a2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grape.embedders import HyperSketching\n",
    "\n",
    "GraphVisualizer(test_kgcovid19, support=train_kgcovid19).fit_and_plot_all(HyperSketching(\n",
    "    # We execute six hops of intersections.\n",
    "    number_of_hops=6\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598136df",
   "metadata": {},
   "source": [
    "## Composing the Bagels\n",
    "Now that we have all ingredients ready, we can proceed to compose our bagels - in fact we are going to create two distinct models, again following an approach similar to what was described in [this paper](https://openreview.net/pdf?id=m1oqEOAozQU). We are going to create a first model which involves trainable graph convolutions for the smaller HPO graph, and another model that uses the precomputed graph convolutions for the larger KGCOVID19 graph.\n",
    "\n",
    "![The Everything Bagel GNN](https://github.com/AnacletoLAB/grape/blob/main/images/bagel.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e8ab1",
   "metadata": {},
   "source": [
    "First thing first, we import the two base models from grape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a519a3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from grape.edge_prediction import GCNEdgePrediction, GNNEdgePrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5205f05",
   "metadata": {},
   "source": [
    "We start with the more complex of the two - the one including convolutional layers. Since the HPO graph includes the important aspect of directionality, we are going to [follow what is suggested in this paper](https://arxiv.org/pdf/2305.10498.pdf) and include the convolution kernels in both directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d94d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = GCNEdgePrediction(\n",
    "    epochs=10,\n",
    "    number_of_units_per_graph_convolution_layers = 32,\n",
    "    number_of_units_per_ffnn_body_layer = 32,\n",
    "    number_of_units_per_ffnn_head_layer = 16,\n",
    "    # We use the two aforementioned kernels\n",
    "    kernels=[\"Symmetric Normalized Laplacian\", \"Transposed Symmetric Normalized Laplacian\"],\n",
    "    dropout_rate=0.7,\n",
    "    # Enable the use of edge metrics as part of the input features, which include:\n",
    "    # - Adamic Adar\n",
    "    # - Jaccard Coefficient\n",
    "    # - Resource allocation index\n",
    "    # - Preferential attachment\n",
    "    use_edge_metrics=True,\n",
    "    # We enable the use of residual graph convolution layers\n",
    "    residual_convolutional_layers=False,\n",
    "    # And the use of a node embedding layer, to allow the network to learn\n",
    "    # its own representation of the nodes - btw you can reuse this\n",
    "    # for other tasks if you want.\n",
    "    use_node_embedding=True,\n",
    "    # And the use of a node type embedding layer, to allow the network to learn\n",
    "    # its own representation of the node types - btw you can reuse this\n",
    "    # for other tasks if you want.\n",
    "    use_node_type_embedding=True,\n",
    "    # And the use of a edge type embedding layer, to allow the network to learn\n",
    "    # its own representation of the node types - btw you can reuse this\n",
    "    # for other tasks if you want.\n",
    "    use_edge_type_embedding=True,\n",
    "    # To combine the node features into edge representation we use two approaches:\n",
    "    # concatenation and hadamard product - though more methods are possible and available.\n",
    "    edge_embedding_methods=[\"Concatenate\", \"Hadamard\"],\n",
    "    # We add the names of the node, node type and edge type features, which solely serve\n",
    "    # to help the visualization of the model and make it a bit clearer.\n",
    "    node_feature_names = [\"SciBERT Nodes\"],\n",
    "    node_type_feature_names = [\"SciBERT Node Types\", \"DeepWalk Node Types\"], \n",
    "    edge_type_feature_names = [\"SciBERT Edge Types\", \"DeepWalk Edge Types\"], \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b2195",
   "metadata": {},
   "source": [
    "We fit the HyperSketching features for the HPO graph - note we are using the training graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3600de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grape.embedders import HyperSketching\n",
    "\n",
    "hpo_hyper_sketching = HyperSketching(\n",
    "    number_of_hops=2,\n",
    ")\n",
    "hpo_hyper_sketching.fit(train_hpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78329a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    graph=train_hpo,\n",
    "    # The support graph is the graph whose topology is to be used for all things\n",
    "    # including the convolutions, the metrics and the edge features.\n",
    "    support=train_hpo,\n",
    "    node_features=[node_bert_embedding_hpo],\n",
    "    node_type_features=[\n",
    "        node_type_bert_embedding_hpo,\n",
    "        node_type_deepwalk_embedding_hpo\n",
    "    ],\n",
    "    edge_type_features=[\n",
    "        edge_type_bert_embedding_hpo,\n",
    "        edge_type_deepwalk_embedding_hpo\n",
    "    ],\n",
    "    #edge_features=[hpo_hyper_sketching]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0481a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba0188",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    graph=train_hpo,\n",
    "    # The support graph is the graph whose topology is to be used for all things\n",
    "    # including the convolutions, the metrics and the edge features.\n",
    "    support=train_hpo,\n",
    "    node_features=[node_bert_embedding_hpo],\n",
    "    node_type_features=[\n",
    "        node_type_bert_embedding_hpo,\n",
    "        node_type_deepwalk_embedding_hpo\n",
    "    ],\n",
    "    edge_type_features=[\n",
    "        edge_type_bert_embedding_hpo,\n",
    "        edge_type_deepwalk_embedding_hpo\n",
    "    ],\n",
    "    edge_features=[hpo_hyper_sketching]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b223a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(\n",
    "    graph=train_hpo,\n",
    "    # The support graph is the graph whose topology is to be used for all things\n",
    "    # including the convolutions, the metrics and the edge features.\n",
    "    support=train_hpo,\n",
    "    node_features=[node_bert_embedding_hpo],\n",
    "    node_type_features=[\n",
    "        node_type_bert_embedding_hpo,\n",
    "        node_type_deepwalk_embedding_hpo\n",
    "    ],\n",
    "    edge_type_features=[\n",
    "        edge_type_bert_embedding_hpo,\n",
    "        edge_type_deepwalk_embedding_hpo\n",
    "    ],\n",
    "    edge_features=[hpo_hyper_sketching],\n",
    "    return_predictions_dataframe=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(\n",
    "    graph=test_hpo,\n",
    "    # The support graph is the graph whose topology is to be used for all things\n",
    "    # including the convolutions, the metrics and the edge features.\n",
    "    support=train_hpo,\n",
    "    node_features=[node_bert_embedding_hpo],\n",
    "    node_type_features=[\n",
    "        node_type_bert_embedding_hpo,\n",
    "        node_type_deepwalk_embedding_hpo\n",
    "    ],\n",
    "    edge_type_features=[\n",
    "        edge_type_bert_embedding_hpo,\n",
    "        edge_type_deepwalk_embedding_hpo\n",
    "    ],\n",
    "    edge_features=[hpo_hyper_sketching],\n",
    "    return_predictions_dataframe=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNNEdgePrediction(\n",
    "    epochs=10,\n",
    "    batch_size=2**17,\n",
    "    number_of_units_per_body_layer=32,\n",
    "    number_of_units_per_head_layer=16,\n",
    "    # Enable the use of edge metrics as part of the input features, which include:\n",
    "    # - Adamic Adar\n",
    "    # - Jaccard Coefficient\n",
    "    # - Resource allocation index\n",
    "    # - Preferential attachment\n",
    "    use_edge_metrics=True,\n",
    "    # And the use of a node embedding layer, to allow the network to learn\n",
    "    # its own representation of the nodes - btw you can reuse this\n",
    "    # for other tasks if you want.\n",
    "    use_node_embedding=True,\n",
    "    # And the use of a node type embedding layer, to allow the network to learn\n",
    "    # its own representation of the node types - btw you can reuse this\n",
    "    # for other tasks if you want.\n",
    "    #use_node_type_embedding=True,\n",
    "    # And the use of a edge type embedding layer, to allow the network to learn\n",
    "    # its own representation of the node types - btw you can reuse this\n",
    "    # for other tasks if you want.\n",
    "    use_edge_type_embedding=True,\n",
    "    # To combine the node features into edge representation we use two approaches:\n",
    "    # concatenation and hadamard product - though more methods are possible and available.\n",
    "    edge_embedding_methods=[\"Concatenate\", \"Hadamard\"],\n",
    "    # We add the names of the node, node type and edge type features, which solely serve\n",
    "    # to help the visualization of the model and make it a bit clearer.\n",
    "    node_feature_names = [\"SciBERT Nodes\"],\n",
    "    node_type_feature_names = [\"SciBERT Node Types\", \"DeepWalk Node Types\"], \n",
    "    edge_type_feature_names = [\"SciBERT Edge Types\", \"DeepWalk Edge Types\"], \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa36e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grape.embedders import HyperSketching\n",
    "\n",
    "kgcovid19_hyper_sketching = HyperSketching(\n",
    "    number_of_hops=3,\n",
    "    precision=8,\n",
    "    bits=6\n",
    ")\n",
    "kgcovid19_hyper_sketching.fit(train_kgcovid19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c578825b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    graph=train_kgcovid19,\n",
    "    # The support graph is the graph whose topology is to be used for all things\n",
    "    # including the convolutions, the metrics and the edge features.\n",
    "    support=train_kgcovid19,\n",
    "    node_features=[conv_node_bert_embedding_kgcovid19_pca],\n",
    "    node_type_features=[\n",
    "        node_type_bert_embedding_kgcovid19_pca,\n",
    "        node_type_deepwalk_embedding_kgcovid19_pca\n",
    "    ],\n",
    "    edge_type_features=[\n",
    "        edge_type_bert_embedding_kgcovid19_pca,\n",
    "        edge_type_deepwalk_embedding_kgcovid19_pca\n",
    "    ],\n",
    "    edge_features=[kgcovid19_hyper_sketching]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebd4fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    graph=train_kgcovid19,\n",
    "    # The support graph is the graph whose topology is to be used for all things\n",
    "    # including the convolutions, the metrics and the edge features.\n",
    "    support=train_kgcovid19,\n",
    "    node_features=[conv_node_bert_embedding_kgcovid19_pca],\n",
    "    node_type_features=[\n",
    "        node_type_bert_embedding_kgcovid19_pca,\n",
    "        node_type_deepwalk_embedding_kgcovid19_pca\n",
    "    ],\n",
    "    edge_type_features=[\n",
    "        edge_type_bert_embedding_kgcovid19_pca,\n",
    "        edge_type_deepwalk_embedding_kgcovid19_pca\n",
    "    ],\n",
    "    edge_features=[kgcovid19_hyper_sketching]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8862359",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(\n",
    "    graph=train_kgcovid19,\n",
    "    # The support graph is the graph whose topology is to be used for all things\n",
    "    # including the convolutions, the metrics and the edge features.\n",
    "    support=train_kgcovid19,\n",
    "    node_features=[conv_node_bert_embedding_kgcovid19_pca],\n",
    "    node_type_features=[\n",
    "        node_type_bert_embedding_kgcovid19_pca,\n",
    "        node_type_deepwalk_embedding_kgcovid19_pca\n",
    "    ],\n",
    "    edge_type_features=[\n",
    "        edge_type_bert_embedding_kgcovid19_pca,\n",
    "        edge_type_deepwalk_embedding_kgcovid19_pca\n",
    "    ],\n",
    "    edge_features=[kgcovid19_hyper_sketching]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e3946",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(\n",
    "    graph=test_kgcovid19,\n",
    "    # The support graph is the graph whose topology is to be used for all things\n",
    "    # including the convolutions, the metrics and the edge features.\n",
    "    support=train_kgcovid19,\n",
    "    node_features=[conv_node_bert_embedding_kgcovid19_pca],\n",
    "    node_type_features=[\n",
    "        node_type_bert_embedding_kgcovid19_pca,\n",
    "        node_type_deepwalk_embedding_kgcovid19_pca\n",
    "    ],\n",
    "    edge_type_features=[\n",
    "        edge_type_bert_embedding_kgcovid19_pca,\n",
    "        edge_type_deepwalk_embedding_kgcovid19_pca\n",
    "    ],\n",
    "    edge_features=[kgcovid19_hyper_sketching]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345826bb",
   "metadata": {},
   "source": [
    "## Future directions\n",
    "In this tutorial we have presented the extremely multi-modal Everything Bagel GNN, though a problem has clearly surfaced - while the model is capable of ingesting many features, it also comes with many free parameters to tune. How can we do that? The typical solutions is hyperparameters optimization. We will explore solutions such as Bayesian Optimization using [the Ray library](https://github.com/ray-project/ray) in the next tutorial!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
